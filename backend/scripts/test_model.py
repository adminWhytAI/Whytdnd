"""
Script de test du mod√®le GGUF (Mistral 7B ou TinyLlama)
"""
import sys
from pathlib import Path

# Ajouter backend au path
sys.path.insert(0, str(Path(__file__).parent.parent))

def test_model():
    """Test le chargement et la g√©n√©ration du mod√®le GGUF"""
    
    print("="*70)
    print("üß™ TEST MOD√àLE GGUF - WhytDD")
    print("="*70)
    
    # Importer config
    try:
        from utils.config import (
            GGUF_MODEL_PATH, 
            CONTEXT_SIZE, 
            N_THREADS,
            N_GPU_LAYERS,
            GGUF_PARAMS,
            verify_setup
        )
    except Exception as e:
        print(f"\n‚ùå Erreur import config: {e}")
        print("\nV√©rifiez que models/config/model_config.json existe")
        return
    
    # V√©rifier setup
    print("\nüìã V√©rification configuration...")
    valid, messages = verify_setup()
    for msg in messages:
        print(f"   {msg}")
    
    if not valid:
        print("\n‚ùå Setup incomplet. Voir messages ci-dessus.")
        return
    
    # Afficher infos mod√®le
    print(f"\nü§ñ Mod√®le GGUF:")
    print(f"   Path: {GGUF_MODEL_PATH}")
    
    if GGUF_MODEL_PATH.exists():
        size_gb = GGUF_MODEL_PATH.stat().st_size / (1024**3)
        print(f"   Taille: {size_gb:.2f} GB")
    else:
        print(f"   ‚ùå FICHIER NON TROUV√â")
        print(f"\n   Pour t√©l√©charger Mistral 7B:")
        print(f"   python backend/scripts/download_mistral.py")
        return
    
    print(f"   Context size: {CONTEXT_SIZE}")
    print(f"   Threads: {N_THREADS}")
    print(f"   GPU Layers: {N_GPU_LAYERS}")
    
    # Charger mod√®le
    print(f"\nüîÑ Chargement du mod√®le...")
    print(f"   (Peut prendre 30-60 secondes...)")
    
    try:
        from llama_cpp import Llama
        
        model = Llama(
            model_path=str(GGUF_MODEL_PATH),
            n_ctx=CONTEXT_SIZE,
            n_threads=N_THREADS,
            n_gpu_layers=N_GPU_LAYERS,
            verbose=False
        )
        
        print(f"   ‚úÖ Mod√®le charg√© avec succ√®s !")
        
    except ImportError:
        print(f"\n‚ùå llama-cpp-python n'est pas install√©")
        print(f"   Installation: pip install llama-cpp-python")
        return
    except Exception as e:
        print(f"\n‚ùå Erreur chargement mod√®le: {e}")
        return
    
    # Test g√©n√©ration simple
    print(f"\nüß™ Test 1 : G√©n√©ration Simple")
    print("-"*70)
    
    prompt = """[INST] Tu es Bruenor, un nain des montagnes guerrier bourru mais loyal.

Utilisateur: Bonjour Bruenor, qui es-tu ?
[/INST]"""
    
    print("Prompt:")
    print(prompt)
    print("\nG√©n√©ration...")
    
    try:
        response = model(
            prompt,
            max_tokens=GGUF_PARAMS['max_tokens'],
            temperature=GGUF_PARAMS['temperature'],
            top_p=GGUF_PARAMS['top_p'],
            top_k=GGUF_PARAMS['top_k'],
            repeat_penalty=GGUF_PARAMS['repeat_penalty'],
            stop=["User:", "\n[INST]", "###"]
        )
        
        generated_text = response['choices'][0]['text'].strip()
        
        print("\n" + "="*70)
        print("üí¨ R√âPONSE G√âN√âR√âE:")
        print("="*70)
        print(generated_text)
        print("="*70)
        
    except Exception as e:
        print(f"\n‚ùå Erreur g√©n√©ration: {e}")
        return
    
    # Test 2 : Avec contexte √©motionnel
    print(f"\n\nüß™ Test 2 : R√©ponse avec Contexte √âmotionnel")
    print("-"*70)
    
    prompt2 = """[INST] Tu es Bruenor, un nain guerrier.

Contexte : Ton clan a √©t√© chass√© de Mithral Hall par des orques il y a 200 ans.
Comportement : Tu d√©testes les orques de mani√®re visc√©rale.

Utilisateur: Que penses-tu des orques ?
[/INST]"""
    
    print("G√©n√©ration...")
    
    try:
        response2 = model(
            prompt2,
            max_tokens=200,
            temperature=0.9,  # Plus de cr√©ativit√©
            top_p=0.9,
            stop=["User:", "\n[INST]"]
        )
        
        generated_text2 = response2['choices'][0]['text'].strip()
        
        print("\n" + "="*70)
        print("üí¨ R√âPONSE G√âN√âR√âE:")
        print("="*70)
        print(generated_text2)
        print("="*70)
        
    except Exception as e:
        print(f"\n‚ùå Erreur g√©n√©ration: {e}")
        return
    
    # R√©sum√©
    print("\n\n" + "="*70)
    print("‚úÖ TESTS TERMIN√âS AVEC SUCC√àS")
    print("="*70)
    print("\nüìä R√©sum√©:")
    print(f"   ‚Ä¢ Mod√®le: {GGUF_MODEL_PATH.name}")
    print(f"   ‚Ä¢ Chargement: OK")
    print(f"   ‚Ä¢ G√©n√©ration: OK")
    print(f"   ‚Ä¢ Roleplay: OK")
    print("\nüéØ Le mod√®le est pr√™t pour WhytDD !")
    print("\nüìù Prochaine √©tape: Cr√©er la documentation immersive")
    print("   Voir TODO.md Phase 1")

if __name__ == "__main__":
    test_model()
